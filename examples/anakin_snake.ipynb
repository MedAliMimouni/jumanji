{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "anakin_snake.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "gpuClass": "standard",
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5ZkbaRsrpAr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Jumanji's Snake with Anakin**\n",
    "\n",
    "This notebook plugs in the Jumanji Snake environment to an Online Deep-Q learning agent following the [Anakin](https://arxiv.org/abs/2104.06272) design. We use the [Podracer Architectures for Scalable RL](https://colab.research.google.com/drive/1974D-qP17fd5mLxy6QZv-ic4yxlPJp-G?usp=sharing) colab by Matteo Hessel, Manuel Kroiss, Fabio Viola, Hado van Hasselt as a basis - most of the code below is copied verbatim with a few small adjustments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0zhqyfea0If",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "! pip install -U pip"
   ],
   "metadata": {
    "id": "8e6_rWelIhZi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: update with pip install of jumanji\n",
    "! pip install --no-cache-dir --no-dependencies git+https://glpat-cnWGWRqf3ohHSZ4HzEG1:glpat-cnWGWRqf3ohHSZ4HzEG1@gitlab.com/instadeep/jumanji.git@develop\n",
    "# Install dependencies\n",
    "! pip install chex -q\n",
    "! pip install dm_haiku -q\n",
    "! pip install rlax -q\n",
    "! pip install optax -q\n",
    "! pip install tqdm -q\n",
    "\n",
    "! pip install brax # TODO: remove"
   ],
   "metadata": {
    "id": "ORKMnvg43FD5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import jax\n",
    "import warnings\n",
    "if jax.devices()[0].platform == 'tpu':\n",
    "    # Setup TPU\n",
    "    import jax.tools.colab_tpu\n",
    "    jax.tools.colab_tpu.setup_tpu()\n",
    "    use_tpu = True\n",
    "    print(\"Running with TPU!\")\n",
    "elif jax.devices()[0].platform == \"gpu\":\n",
    "    use_tpu = False\n",
    "    print(\"Running with GPU!\")\n",
    "else:\n",
    "    use_tpu = False\n",
    "    print(\"Running with GPU!\")\n",
    "    warnings.warn(\"Running with CPU. We reccomend running this notebook in Colab with TPU enabled,\"\n",
    "              \"or GPU if TPU is not available.\")\n",
    "\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "import chex\n",
    "import jax\n",
    "import haiku as hk\n",
    "from jax import lax\n",
    "from jax import random\n",
    "from jax import numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import rlax\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Jumanji imports\n",
    "from jumanji import Environment\n",
    "import jumanji\n",
    "from jumanji.wrappers import AutoResetWrapper"
   ],
   "metadata": {
    "id": "YwZ6Mf9b22qz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Class for timing blocks of code.\n",
    "class TimeIt():\n",
    "    def __init__(self, tag, frames=None):\n",
    "        self.tag = tag\n",
    "        self.frames = frames\n",
    "\n",
    "    def __enter__(self):\n",
    "      self.start = timeit.default_timer()\n",
    "      return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.elapsed_secs = timeit.default_timer() - self.start\n",
    "        msg = self.tag + (': Elapsed time=%.2fs' % self.elapsed_secs)\n",
    "        if self.frames:\n",
    "            msg += ', FPS=%.2e' % (self.frames / self.elapsed_secs)\n",
    "            print(msg)"
   ],
   "metadata": {
    "id": "dJJkQtSWlv3s",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qsxn_iLXa5S9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Anakin\n",
    "We follow the code from the [Podracer Architectures for Scalable RL](https://colab.research.google.com/drive/1974D-qP17fd5mLxy6QZv-ic4yxlPJp-G?usp=sharing) colab with the folowing additions:\n",
    "\n",
    "\n",
    "*   Epsilon greedy action selection (instead of greedy action selection).\n",
    "*   We record metrics from interation with the environment, such as the mean and max episode return across each batch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define key containers"
   ],
   "metadata": {
    "id": "eu2xIOeMVR-k",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "@chex.dataclass(frozen=True)\n",
    "class TimeStep:\n",
    "    \"\"\"Data used for training.\"\"\"\n",
    "\n",
    "    q_values: chex.Array\n",
    "    action: chex.Array\n",
    "    discount: chex.Array\n",
    "    reward: chex.Array\n",
    "\n",
    "@chex.dataclass(frozen=True)\n",
    "class EpisodeMetrics:\n",
    "    \"\"\"Metrics that we use to keep track of the episode return and length \n",
    "    throughout interation with the environment.\"\"\"\n",
    "\n",
    "    episode_return: chex.Array\n",
    "    episode_length: chex.Array"
   ],
   "metadata": {
    "id": "rMQl_mCn2OdB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the Deep-Q network"
   ],
   "metadata": {
    "id": "1v_7Cf6NVaJN",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_network_fn(num_outputs: int):\n",
    "    \"\"\"Define conv net for agent.\"\"\"\n",
    "\n",
    "    def network_fn(obs: chex.Array) -> chex.Array:\n",
    "        return hk.Sequential(\n",
    "            [\n",
    "                hk.Conv2D(32, (2, 2), 2),\n",
    "                jax.nn.relu,\n",
    "                hk.Conv2D(32, (2, 2), 1),\n",
    "                jax.nn.relu,\n",
    "                hk.Flatten(),\n",
    "                hk.Linear(64),\n",
    "                jax.nn.relu,\n",
    "                hk.Linear(64),\n",
    "                jax.nn.relu,\n",
    "                hk.Linear(num_outputs),\n",
    "            ]\n",
    "        )(obs)\n",
    "\n",
    "    return hk.without_apply_rng(hk.transform(network_fn))"
   ],
   "metadata": {
    "id": "Gy4VcJv12XzZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the learner function\n",
    "This contains the following key functionality:\n",
    "*   Collect experience for a single trajectory, recording any useful information.\n",
    "*   Calculate the Q-learning loss for the trajectory.\n",
    "*   Calculate the gradient of the loss with respect to the parameters of the network.\n",
    "*   Aggregate the gradient across a batch of trajectories, and across multiple devices (if using TPU). \n",
    "*   Update the parameters of the network using the gradient.\n",
    "*   Aggregate useful information (such as the episode return) for visualising training.\n",
    "*   Run multiple updates to the agent parameters without going back to python using `jax.lax.scan`. The original Anakin implementation uses `jax.lax.fori` for this. However as we would like to obtain information from each step of the algoirthm for visualisation of training we use `jax.lax.scan`. "
   ],
   "metadata": {
    "id": "C2xpCarf7M_0",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mGSmAiCHJsas",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def get_learner_fn(\n",
    "    env,\n",
    "    forward_pass,\n",
    "    opt_update,\n",
    "    rollout_len,\n",
    "    agent_discount,\n",
    "    lambda_,\n",
    "    iterations,\n",
    "    epsilon,\n",
    "):\n",
    "    \"\"\"Define the minimal unit of computation in Anakin.\"\"\"\n",
    "\n",
    "    def loss_fn(params, outer_rng, env_state, prev_obs, episode_metrics):\n",
    "        \"\"\"Compute the loss on a single trajectory.\"\"\"\n",
    "\n",
    "        def step_fn(carry, rng):\n",
    "            env_state, prev_obs, episode_metrics = carry\n",
    "            q_values = forward_pass(params, prev_obs[None,])[\n",
    "                0\n",
    "            ]  # forward pass.\n",
    "            action = rlax.epsilon_greedy(epsilon).sample(rng, q_values)\n",
    "            env_state, env_timestep, extra = env.step(\n",
    "                env_state, action\n",
    "            )  # step environment.\n",
    "\n",
    "            obs = env_timestep.observation\n",
    "            discount = env_timestep.discount\n",
    "            reward = env_timestep.reward\n",
    "            # record info that we will log\n",
    "            info = {\n",
    "                \"episode_return\": jnp.where(\n",
    "                    discount == 0.0, reward + episode_metrics.episode_return, jnp.nan\n",
    "                ),\n",
    "                \"episode_length\": jnp.where(\n",
    "                    discount == 0.0, 1 + episode_metrics.episode_length, jnp.nan\n",
    "                ),\n",
    "            }\n",
    "            # update the episode_metrics that we use to keep track of the \n",
    "            # episode return and length.\n",
    "            episode_metrics = EpisodeMetrics(\n",
    "                episode_return=jnp.where(\n",
    "                    discount == 0.0,\n",
    "                    jnp.array(0.0),\n",
    "                    reward + episode_metrics.episode_return,\n",
    "                ),\n",
    "                episode_length=jnp.where(\n",
    "                    discount == 0.0, jnp.array(0), 1 + episode_metrics.episode_length\n",
    "                ),\n",
    "            )\n",
    "            carry = env_state, obs, episode_metrics\n",
    "            return carry, (\n",
    "                TimeStep(  # return env state and transition data.\n",
    "                    q_values=q_values,\n",
    "                    action=action,\n",
    "                    discount=discount,\n",
    "                    reward=reward,\n",
    "                ),\n",
    "                info,\n",
    "            )\n",
    "\n",
    "        step_rngs = random.split(outer_rng, rollout_len)\n",
    "        (env_state, prev_obs, episode_metrics), (rollout, info) = lax.scan(\n",
    "            step_fn, (env_state, prev_obs, episode_metrics), step_rngs\n",
    "        )  # trajectory.\n",
    "        qa_tm1 = rlax.batched_index(rollout.q_values[:-1], rollout.action[:-1])\n",
    "        td_error = rlax.td_lambda(  # compute multi-step temporal diff error.\n",
    "            v_tm1=qa_tm1,  # predictions.\n",
    "            r_t=rollout.reward[:-1],  # rewards.\n",
    "            discount_t=agent_discount * rollout.discount[:-1],  # discount.\n",
    "            v_t=jnp.max(rollout.q_values[1:], axis=-1),  # bootstrap values.\n",
    "            lambda_=lambda_,\n",
    "        )  # mixing hyper-parameter lambda.\n",
    "        # record info for visualising training progress.\n",
    "        info = {\n",
    "            \"episode_return\": jnp.nanmean(info[\"episode_return\"], axis=0),\n",
    "            \"episode_length\": jnp.nanmean(info[\"episode_length\"], axis=0),\n",
    "            \"reward\": jnp.mean(rollout.reward),\n",
    "            \"q_values\": jnp.mean(qa_tm1),\n",
    "            \"max_episode_return\": jnp.nanmax(info[\"episode_return\"], axis=0)\n",
    "        }\n",
    "        return jnp.mean(td_error**2), (env_state, prev_obs, episode_metrics, info)\n",
    "\n",
    "    def update_fn(params, opt_state, rng, env_state, prev_obs, episode_metrics):\n",
    "        \"\"\"Compute a gradient update from a single trajectory.\"\"\"\n",
    "        rng, loss_rng = random.split(rng)\n",
    "        grads, (\n",
    "            new_env_state,\n",
    "            new_prev_obs,\n",
    "            episode_metrics,\n",
    "            info,\n",
    "        ) = jax.grad(  # compute gradient on a single trajectory.\n",
    "            loss_fn, has_aux=True\n",
    "        )(\n",
    "            params, loss_rng, env_state, prev_obs, episode_metrics\n",
    "        )\n",
    "        grads = lax.pmean(grads, axis_name=\"j\")  # reduce mean across cores.\n",
    "        grads = lax.pmean(grads, axis_name=\"i\")  # reduce mean across batch.\n",
    "        updates, new_opt_state = opt_update(grads, opt_state)  # transform grads.\n",
    "        new_params = optax.apply_updates(params, updates)  # update parameters.\n",
    "        return (\n",
    "            new_params,\n",
    "            new_opt_state,\n",
    "            rng,\n",
    "            new_env_state,\n",
    "            new_prev_obs,\n",
    "            episode_metrics,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def learner_fn(params, opt_state, rngs, env_states, prev_obses, episode_metrics):\n",
    "        \"\"\"Vectorise and repeat the update.\"\"\"\n",
    "        batched_update_fn = jax.vmap(\n",
    "            update_fn, axis_name=\"j\"\n",
    "        )  # vectorize across batch.\n",
    "\n",
    "        def iterate_fn(carry, _):  # repeat many times to avoid going back to Python.\n",
    "            params, opt_state, rngs, env_states, prev_obses, episode_return = carry\n",
    "            (\n",
    "                new_params,\n",
    "                new_opt_state,\n",
    "                rng,\n",
    "                new_env_state,\n",
    "                new_prev_obs,\n",
    "                episode_return,\n",
    "                info,\n",
    "            ) = batched_update_fn(\n",
    "                params, opt_state, rngs, env_states, prev_obses, episode_return\n",
    "            )\n",
    "            carry = (\n",
    "                new_params,\n",
    "                new_opt_state,\n",
    "                rng,\n",
    "                new_env_state,\n",
    "                new_prev_obs,\n",
    "                episode_return,\n",
    "            )\n",
    "            # record mean of all info over the batch, as well as the max return.\n",
    "            max_return_info = {\"max_episode_return\": \n",
    "                               jnp.nanmax(info[\"max_episode_return\"], axis=0)}\n",
    "            info = jax.tree_util.tree_map(lambda x: jnp.nanmean(x, axis=0), info)\n",
    "            info.update(max_return_info)\n",
    "            return carry, info\n",
    "\n",
    "        init = params, opt_state, rngs, env_states, prev_obses, episode_metrics\n",
    "        return jax.lax.scan(iterate_fn, init, xs=None, length=iterations)\n",
    "\n",
    "    return learner_fn\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the experiment\n",
    "Sets up a run of the agent given a set of hyper-parameters.\n",
    "The experiment runs multiple pmapped iterations of the `learner_fn` until the desired total number of training iterations has been reached. "
   ],
   "metadata": {
    "id": "m1cwvovq7Qmp",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def run_experiment(\n",
    "    env,\n",
    "    batch_size,\n",
    "    rollout_len,\n",
    "    step_size,\n",
    "    iterations,\n",
    "    seed,\n",
    "    epsilon = 0.02,\n",
    "    agent_discount=0.99,\n",
    "    lambda_=0.95,\n",
    "    inner_iter_length=100, \n",
    "):\n",
    "    \"\"\"Runs experiment. \n",
    "\n",
    "    `inner_iter_length` sets how many iterations to do in the `jax.lax.scan`\n",
    "    before going back to python\n",
    "    \"\"\"\n",
    "    cores_count = len(jax.devices())  # get available TPU cores.\n",
    "    network = get_network_fn(env.action_spec().num_values)  # define network.\n",
    "    optim = optax.adam(step_size)  # define optimiser.\n",
    "\n",
    "    rng, rng_e, rng_p = random.split(random.PRNGKey(seed), num=3)  # prng keys.\n",
    "    dummy_obs = env.observation_spec().generate_value()[\n",
    "        None,\n",
    "    ]  # dummy for net init.\n",
    "    params = network.init(rng_p, dummy_obs)  # initialise params.\n",
    "    opt_state = optim.init(params)  # initialise optimiser stats.\n",
    "\n",
    "\n",
    "    learn = get_learner_fn(  # get batched iterated update.\n",
    "        env,\n",
    "        network.apply,\n",
    "        optim.update,\n",
    "        rollout_len=rollout_len,\n",
    "        agent_discount=agent_discount,\n",
    "        lambda_=lambda_,\n",
    "        iterations=inner_iter_length,\n",
    "        epsilon=epsilon\n",
    "    )\n",
    "    learn = jax.pmap(learn, axis_name=\"i\")  # replicate over multiple cores.\n",
    "\n",
    "    broadcast = lambda x: jnp.broadcast_to(x, (cores_count, batch_size) + x.shape)\n",
    "    params = jax.tree_map(broadcast, params)  # broadcast to cores and batch.\n",
    "    opt_state = jax.tree_map(broadcast, opt_state)  # broadcast to cores and batch\n",
    "\n",
    "    episode_metrics = EpisodeMetrics(\n",
    "            episode_return=jnp.zeros(shape=(), dtype=jnp.float32),\n",
    "            episode_length=jnp.zeros(shape=(), dtype=jnp.int16),\n",
    "        )\n",
    "    episode_metrics = jax.tree_map(broadcast, episode_metrics)\n",
    "\n",
    "    rng, *env_rngs = jax.random.split(rng, cores_count * batch_size + 1)\n",
    "    env_states, timesteps, _ = jax.vmap(env.reset)(jnp.stack(env_rngs))  # init envs.\n",
    "    rng, *step_rngs = jax.random.split(rng, cores_count * batch_size + 1)\n",
    "\n",
    "    reshape = lambda x: jax.tree_util.tree_map(\n",
    "        lambda x: x.reshape((cores_count, batch_size) + x.shape[1:]), x\n",
    "    )\n",
    "    step_rngs = reshape(jnp.stack(step_rngs))  # add dimension to pmap over.\n",
    "    env_states = reshape(env_states)  # add dimension to pmap over.\n",
    "    prev_obs = reshape(timesteps.observation)\n",
    "\n",
    "    num_frames_compile = cores_count * inner_iter_length * rollout_len * batch_size\n",
    "    with TimeIt(tag=\"COMPILATION\", frames=num_frames_compile):\n",
    "        learn(params, opt_state, step_rngs, env_states, prev_obs, episode_metrics)  # compiles\n",
    "\n",
    "    num_frames = cores_count * iterations * rollout_len * batch_size\n",
    "    n_outer_iter = int(iterations // inner_iter_length)\n",
    "    with TimeIt(tag=\"EXECUTION\", frames=num_frames):\n",
    "        for i in tqdm(range(n_outer_iter)):\n",
    "            (\n",
    "                params,\n",
    "                opt_state,\n",
    "                step_rngs,\n",
    "                env_states,\n",
    "                prev_obs,\n",
    "                episode_metrics,\n",
    "            ), new_info = learn(  # runs compiled fn\n",
    "                params, opt_state, step_rngs, env_states, prev_obs, \n",
    "                episode_metrics\n",
    "            )\n",
    "            # record metrics\n",
    "            max_return_info = {\"max_episode_return\": \n",
    "                               jnp.nanmax(new_info[\"max_episode_return\"], axis=0)}\n",
    "            new_info = jax.tree_util.tree_map(lambda x: jnp.nanmean(x, axis=0), new_info)\n",
    "            new_info.update(max_return_info)\n",
    "            if i == 0:\n",
    "                info = new_info\n",
    "            else:\n",
    "                info = {key: jnp.concatenate((info[key], new_info[key])) for key in info.keys()}\n",
    "    params_single_device = jax.tree_util.tree_map(lambda x: x[0, 0], params)\n",
    "    return params_single_device, info"
   ],
   "metadata": {
    "id": "nxbBbymnyB2X",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run experiment and visualise results"
   ],
   "metadata": {
    "id": "IC6rqMDkA4Ri",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Running on\", len(jax.devices()), \"cores.\", flush=True)  # !expected 8!\n",
    "# We auto-reset the environment state when the episode ends during training.\n",
    "env = AutoResetWrapper(jumanji.make('Snake-6x6-v0'))\n",
    "# Define hyper-parameters.\n",
    "batch_size = 256\n",
    "seed = 0\n",
    "rollout_len = 8\n",
    "step_size = 1e-3 # learning rate\n",
    "if use_tpu: \n",
    "  # The TPU has 8 devices (each vmapping over batch_size many environments).\n",
    "  # Therefore we need less training iterations to obtain good performance.\n",
    "  iterations = 8000\n",
    "else: \n",
    "  # For GPU we can do less in parellel and \n",
    "  # therefore need more training iterations.\n",
    "  iterations = 20000\n",
    "\n",
    "params, info = run_experiment(env=env, batch_size=batch_size, rollout_len=rollout_len, step_size=step_size, iterations=iterations, seed=seed)"
   ],
   "metadata": {
    "id": "JrdqPcM22_r0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the mean and max return throughout training.\n",
    "# The maximum return possible on Snake-6x6-v0 is 35.\n",
    "# We see that by the end of the episode the agent is regularly obtaining \n",
    "# the max return.\n",
    "# Some other keys in info that can also be plotted: \"q_values\", \"reward\", \"episode_length\"\n",
    "\n",
    "plt.plot(info[\"max_episode_return\"])\n",
    "plt.title(\"max episode return per batch\")\n",
    "plt.xlabel(\"training iteration\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(info[\"episode_return\"])\n",
    "plt.title(\"mean episode return per batch\")\n",
    "plt.xlabel(\"training iteration\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "KLrkexHtrF7F",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Now we run an episode using greedy action selection while rendering the environment.\n",
    "# The trained agent obtains close to the maximum return (35). \n",
    "\n",
    "# Re-instantiate the environment without the auto-reset.\n",
    "# TODO: Add render once it has been set up for snake.\n",
    "env = jumanji.make('Snake-6x6-v0')\n",
    "key = jax.random.PRNGKey(0)\n",
    "network = get_network_fn(env.action_spec().num_values)\n",
    "\n",
    "state, timestep, _ = env.reset(key)\n",
    "episode_return = 0\n",
    "while not timestep.last():\n",
    "    key, subkey = jax.random.split(key)\n",
    "    q_values = network.apply(params, timestep.observation[None, ])[0]  # forward pass.\n",
    "    action = jnp.argmax(q_values)  # greedy policy.\n",
    "    state, timestep, extra = env.step(state, action)  # step environment\n",
    "    episode_return += timestep.reward\n",
    "    # env.render(state)\n",
    "print(f\"episode complete with return of {episode_return}\")"
   ],
   "metadata": {
    "id": "mNd-1Zgp5MGZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
